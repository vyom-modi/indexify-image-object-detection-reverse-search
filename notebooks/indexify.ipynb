{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **YOLO Image Object Detection with Indexify**\n","\n","This guide demonstrates how to create an image object detection pipeline using Indexify and the tensorlake/yolo extractor. By following this guide, you'll build a pipeline capable of ingesting image files and detecting objects within them using the YOLO (You Only Look Once) model."]},{"cell_type":"markdown","metadata":{},"source":["## **Table of Contents**\n","\n","1. [Introduction](#introduction)\n","2. [Prerequisites](#prerequisites)\n","3. [Setup](#setup)\n","   - [Install Indexify](#install-indexify)\n","   - [Install Required Extractor](#install-required-extractor)\n","4. [Creating the Extraction Graph](#creating-the-extraction-graph)\n","5. [Implementing the Object Detection Pipeline](#implementing-the-object-detection-pipeline)\n","6. [Running the Object Detection Process](#running-the-object-detection-process)\n","7. [Customization and Advanced Usage](#customization-and-advanced-usage)\n","8. [Conclusion](#conclusion)"]},{"cell_type":"markdown","metadata":{},"source":["## **Introduction**\n","\n","This pipeline uses the `tensorlake/yolo-extractor` to process images and identify objects, providing bounding boxes, class names, and confidence scores for each detected object."]},{"cell_type":"markdown","metadata":{},"source":["## **Prerequisites**\n","\n","Ensure you have the following before starting:\n","\n","\n","\n","    - A virtual environment with Python 3.9 or later\n","    - `pip` (Python package manager)\n","    - Basic familiarity with Python and command-line interfaces\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python3.9 -m venv ve\n","!source ve/bin/activate"]},{"cell_type":"markdown","metadata":{},"source":["# **Training the Model on the Natural-Images dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T12:53:36.545430Z","iopub.status.busy":"2024-07-16T12:53:36.545048Z","iopub.status.idle":"2024-07-16T12:53:50.666805Z","shell.execute_reply":"2024-07-16T12:53:50.665486Z","shell.execute_reply.started":"2024-07-16T12:53:36.545399Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["!pip install -U ipywidgets"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T12:53:50.669315Z","iopub.status.busy":"2024-07-16T12:53:50.668962Z","iopub.status.idle":"2024-07-16T12:54:03.990475Z","shell.execute_reply":"2024-07-16T12:54:03.989221Z","shell.execute_reply.started":"2024-07-16T12:53:50.669280Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["!pip install ultralytics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T12:54:07.283764Z","iopub.status.busy":"2024-07-16T12:54:07.283296Z","iopub.status.idle":"2024-07-16T12:54:07.291096Z","shell.execute_reply":"2024-07-16T12:54:07.289116Z","shell.execute_reply.started":"2024-07-16T12:54:07.283697Z"},"trusted":true},"outputs":[],"source":["with open('fashion_images.yaml', 'w') as yaml_handle:\n","    yaml_handle.write(\"\"\"\n","    # fashion_images.yaml\n","\n","    # Paths to the dataset\n","    train: /workspace/image-object-detection-reverse-search/data\n","    val: /workspace/image-object-detection-reverse-search/data\n","\n","    # Number of classes\n","    nc: 4  # number of classes\n","    names: [\"Boys\", \"Girls\", \"Men\", \"Women\"]\n","\n","    # Model architecture (example for YOLOv8)\n","    backbone:\n","    - [type, model, pretrained, stride]\n","    - [Conv, 32, 3, 1]       # Convolutional layer\n","    - [Conv, 64, 3, 2]       # Convolutional layer with stride 2\n","    - [CSP, 64, 1, 1]        # CSP block\n","    - [CSP, 128, 3, 2]       # CSP block with stride 2\n","    - [CSP, 256, 3, 2]       # CSP block with stride 2\n","    - [CSP, 512, 1, 1]       # CSP block\n","\n","    head:\n","    - [type, model, pretrained, stride]\n","    - [CSP, 512, 1, 1]       # CSP block\n","    - [SPPF, 512, 1, 1]      # SPPF block\n","    - [Conv, 256, 1, 1]      # Convolutional layer\n","    - [Conv, 128, 1, 1]      # Convolutional layer\n","    - [Detect, 4, 1, 1]      # Detection layer for 4 classes\n","    \"\"\")\n","    yaml_handle.close()\n","\n","# train: /kaggle/input/fashion-images/data\n","#     val: /kaggle/input/fashion-images/data\n","\n","#     nc: 4  # number of classes\n","#     names: [\"Boys\", \"Girls\", \"Men\", \"Women\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T12:54:09.282196Z","iopub.status.busy":"2024-07-16T12:54:09.281266Z","iopub.status.idle":"2024-07-16T13:04:54.805135Z","shell.execute_reply":"2024-07-16T13:04:54.801524Z","shell.execute_reply.started":"2024-07-16T12:54:09.282161Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["from ultralytics import YOLO\n","\n","# Load a model\n","model = YOLO(\"yolov8n.pt\", task='classify')  # load a pretrained model (recommended for training)\n","# model = YOLO(\"/workspace/image-object-detection-reverse-search/notebooks/fashion_images.yaml\", task='detect')  # build a new model from scratch\n","\n","# Use the model\n","model.train(data=\"fashion_images.yaml\", epochs=3)  # train the model\n","# metrics = model.val()  # evaluate model performance on the validation set\n","# results = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\n","# path = model.export(format=\"onnx\")  # export the model to ONNX format (you can choose the format suitable to us here)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:18:37.089180Z","iopub.status.busy":"2024-07-16T13:18:37.088032Z","iopub.status.idle":"2024-07-16T13:18:37.340972Z","shell.execute_reply":"2024-07-16T13:18:37.339967Z","shell.execute_reply.started":"2024-07-16T13:18:37.089141Z"},"trusted":true},"outputs":[],"source":["model.save(\"yolov8n_trained.pt\")"]},{"cell_type":"markdown","metadata":{},"source":["## **Setup**\n","\n","### **Install Indexify**\n","\n","First, install Indexify using the official installation script and start the server:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!curl https://getindexify.ai | sh ./indexify server -d"]},{"cell_type":"markdown","metadata":{},"source":["This starts a long-running server that exposes ingestion and retrieval APIs to applications."]},{"cell_type":"markdown","metadata":{},"source":["### **Install Required Extractor**\n","\n","Next, install the YOLO extractor in a new terminal and start it:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install indexify-extractor-sdk\n","!indexify-extractor download tensorlake/yolo-extractor\n","!indexify-extractor join-server"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["!pip install indexify"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","This script demonstrates how to define and create an extraction graph for image object detection using Indexify and the YOLO model.\n","\n","1. Import necessary modules from the Indexify library.\n","2. Initialize the Indexify client.\n","3. Define the extraction graph specification in YAML format.\n","4. Create an ExtractionGraph object from the YAML specification.\n","5. Use the Indexify client to create the extraction graph in the Indexify system.\n","\"\"\"\n","\n","from indexify import IndexifyClient, ExtractionGraph  # Import Indexify classes\n","\n","# Step 2: Initialize the Indexify client\n","client = IndexifyClient()\n","\n","# Step 3: Define the extraction graph specification using YAML\n","extraction_graph_spec = \"\"\"\n","name: 'yolo_detector'  # Name of the extraction graph\n","extraction_policies:\n","  - extractor: 'tensorlake/yolo-extractor'  # Use YOLO extractor for object detection\n","    name: 'image_object_detection'  # Name of the extraction policy\n","    input_params:\n","      model_name: 'yolov8n_trained.pt'  # Specify the model to be used\n","      conf: 0.25  # Confidence threshold for object detection\n","      iou: 0.7  # Intersection over Union threshold for non-max suppression\n","\"\"\"\n","\n","# Step 4: Create an ExtractionGraph object from the YAML specification\n","extraction_graph = ExtractionGraph.from_yaml(extraction_graph_spec)\n","\n","# Step 5: Create the extraction graph in the Indexify system\n","client.create_extraction_graph(extraction_graph)"]},{"cell_type":"markdown","metadata":{},"source":["## **Creating the Extraction Graph**\n","\n","The extraction graph defines the flow of data through various components. You will need to configure this graph to use the YOLO extractor for object detection."]},{"cell_type":"markdown","metadata":{},"source":["## **Implementing the Object Detection Pipeline**\n","Now that we have our extraction graph set up, we can upload images and make the pipeline detect objects."]},{"cell_type":"markdown","metadata":{},"source":["### **Code Explanation**"]},{"cell_type":"markdown","metadata":{},"source":["1. **detect_objects Function**:\n","\n","- The `detect_objects` function uses the Indexify service to detect objects in a single image. Hereâ€™s a brief overview of the process:\n","\n","   - **Initialization**: The Indexify client is initialized.\n","   - **Image Upload**: The image file is uploaded to the \"yolo_image_detector\" extraction graph.\n","   - **Extraction Process**: The function waits for the Indexify service to complete the extraction process.\n","   - **Retrieve Detections**: The detected objects are retrieved from the extraction graph, which includes information such as bounding boxes, classes, and confidence scores."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import json\n","import os\n","from indexify import IndexifyClient\n","\n","def detect_objects(image_path):\n","    \"\"\"\n","    Detect objects in an image using the Indexify service.\n","\n","    Parameters:\n","    image_path (str): The path to the image file to be processed.\n","\n","    Returns:\n","    list: A list of detected objects with their bounding boxes, classes, and confidence scores.\n","    \"\"\"\n","    # Initialize the Indexify client\n","    client = IndexifyClient()\n","\n","    # Upload the image file to the extraction graph \"yolo_image_detector\"\n","    # This sends the image to the Indexify service for object detection\n","    content_id = client.upload_file(\"yolo_detector\", image_path)\n","    print(f\"Uploaded image, content ID: {content_id}\")\n","\n","    # Wait for the extraction process to complete\n","    # This ensures the service has finished processing the image\n","    client.wait_for_extraction(content_id)\n","    print(f\"Extraction completed for content ID: {content_id}\")\n","\n","    # Retrieve the detected objects from the extraction graph\n","    # This gets the results of the object detection process\n","    detections = client.get_extracted_content(\n","        content_id=content_id,\n","        graph_name=\"yolo_detector\",\n","        policy_name=\"image_object_detection\"\n","    )\n","    print(f\"Detections retrieved for content ID: {content_id}\")\n","\n","    # Return the detections\n","    # Uncomment the following line to format the detections into a more readable format\n","    # return [{'bbox': det['bbox'], 'class': det['class'], 'confidence': det['features'][0]['metadata']['score']} for det in detections]\n","\n","    return detections"]},{"cell_type":"markdown","metadata":{},"source":["#### **Code Example to Detect Objects in a Single Image**"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["detections = detect_objects('/workspace/image-object-detection-reverse-search/sample.jpg')\n","\n","for detection in detections:\n","    content = json.loads(detection['content'])\n","    bbox = content['bbox']\n","    object_class = content['class']\n","    score = content['score']\n","    print(f\"ID: {detection['id']}\")\n","    print(f\"Class: {object_class}\")\n","    print(f\"Score: {score:.2f}\")\n","    print(f\"Bounding Box: {bbox}\")\n","    print(\"=\"*40)"]},{"cell_type":"markdown","metadata":{},"source":["2. **process_dataset Function**: \n","   - Walks through the `natural_images` dataset directory structure.\n","   - Iterates through each image file in each category folder.\n","   - Calls `detect_objects` for each image and prints the detection results."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["def process_dataset(dataset_folder):\n","    \"\"\"\n","    Process a dataset of images, detect objects in each image, and print the details of detected objects.\n","\n","    Parameters:\n","    dataset_folder (str): The path to the folder containing the dataset of images.\n","    \"\"\"\n","    # Traverse the dataset folder recursively\n","    for root, subdirs, files in os.walk(dataset_folder):\n","        for subdir in subdirs:\n","            subdir_path = os.path.join(root, subdir)\n","            print(f\"Processing folder: {subdir_path}\")\n","            # Iterate through each file in the subdirectory\n","            for filename in os.listdir(subdir_path):\n","                file_path = os.path.join(subdir_path, filename)\n","                try:\n","                    # Detect objects in the current image\n","                    detections = detect_objects(file_path)\n","                    print(f\"Image: {filename}\")\n","                    print(f\"Number of objects detected: {len(detections)}\")\n","                    # Print details of each detected object\n","                    for detection in detections:\n","                        content = json.loads(detection['content'])\n","                        bbox = content['bbox']\n","                        object_class = content['class']\n","                        score = content['score']\n","                        print(f\"ID: {detection['id']}\")\n","                        print(f\"Class: {object_class}\")\n","                        print(f\"Score: {score:.2f}\")\n","                        print(f\"Bounding Box: {bbox}\")\n","                        print(\"=\"*40)\n","                    print(\"\\n\")\n","                except Exception as e:\n","                    # Print an error message if the image processing fails\n","                    print(f\"Failed to process {file_path}: {e}\")\n","\n","if __name__ == \"__main__\":\n","    # Define the path to the dataset folder\n","    dataset_folder = '/workspace/image-object-detection-reverse-search/data'\n","    # Process the dataset\n","    process_dataset(dataset_folder)"]},{"cell_type":"markdown","metadata":{},"source":["## **Reverse Image Search Pipeline**"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["!pip install qdrant_client"]},{"cell_type":"markdown","metadata":{},"source":["1. **Initialize the Qdrant client and Create a collection named \"image_search\" in Qdrant**\n","\n","- First, we need to set up the connection to the Qdrant server and create a collection that will store our image vectors. This collection will be used for efficient image search."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import json\n","import uuid\n","from qdrant_client import QdrantClient\n","from qdrant_client.http import models\n","from indexify import IndexifyClient\n","\n","# Initialize the Qdrant client, connecting to the local Qdrant server running on port 6333\n","client = QdrantClient('http://localhost:6333')\n","\n","# Create a collection named \"image_search\" in Qdrant\n","# The collection will store vectors with a dimensionality (size) of 5\n","# The vectors will use dot product for similarity measurement by default, we can change the same using distance attribute\n","client.create_collection(\n","    collection_name=\"image_search\",\n","    vectors_config=models.VectorParams(size=5),\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["2. **Extract features from an image using the Indexify service.**\n","\n","- This step involves utilizing the Indexify service to extract meaningful features from an image. These features can include object detections, scene descriptions, or any other relevant information extracted from the image data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_image_features(image_path):\n","    \"\"\"\n","    Extract features from an image using the Indexify service.\n","\n","    Parameters:\n","    image_path (str): The path to the image file to be processed.\n","\n","    Returns:\n","    tuple: A tuple containing the extracted features and the content ID of the image.\n","    \"\"\"\n","    # Initialize the Indexify client\n","    client = IndexifyClient()\n","\n","    # Upload the image file to the extraction graph \"yolo_image_detector\"\n","    # This sends the image to the Indexify service for feature extraction\n","    content_id = client.upload_file(\"yolo_image_detector\", image_path)\n","    print(f\"Waiting for extraction to complete for content id: {content_id}\")\n","\n","    # Wait for the extraction process to complete\n","    # This ensures the features are fully extracted before proceeding\n","    client.wait_for_extraction(content_id)\n","    print(f\"Extraction completed for content id: {content_id}\")\n","\n","    # Retrieve the extracted features from the extraction graph\n","    features = client.get_extracted_content(\n","        content_id=content_id,\n","        graph_name=\"yolo_image_detector\",\n","        policy_name=\"image_object_detection\"\n","    )\n","\n","    # Return the extracted features and the content ID\n","    return features, content_id"]},{"cell_type":"markdown","metadata":{},"source":["3. **Store extracted image features in the Qdrant vector store.**\n","- After extracting features from an image using services like Indexify, the next step is to store these extracted features in the Qdrant vector store. This allows for efficient storage and retrieval of image vectors, facilitating tasks such as similarity searches and **reverse image lookups**."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def store_features_in_vector_store(features, content_id):\n","    \"\"\"\n","    Store extracted image features in the Qdrant vector store.\n","\n","    Parameters:\n","    features (list): A list of extracted features from an image.\n","    content_id (str): The content ID of the image from which features were extracted.\n","    \"\"\"\n","    # Initialize the Qdrant client, connecting to the local Qdrant server\n","    client = QdrantClient(\"http://localhost:6333\")\n","    points = []\n","\n","    for feature in features:\n","        # Extract the feature content and parse it as JSON\n","        feature_content = feature['content']\n","        feature_data = json.loads(feature_content.decode('utf-8'))\n","\n","        # Extract the bounding box, class name, and score from the feature data\n","        bbox = feature_data['bbox']\n","        class_name = feature_data['class']\n","        score = feature_data['score']\n","\n","        # Create a vector representation combining bbox coordinates and score\n","        # Assuming bbox is a list of numbers (e.g., [x, y, width, height])\n","        vector = bbox + [score]\n","\n","        # Generate a new UUID for each point\n","        point_id = str(uuid.uuid4())\n","        \n","        # Add the point with payload including the original content_id and class name\n","        points.append(models.PointStruct(\n","            id=point_id, \n","            vector=vector, \n","            payload={\"class\": class_name, \"content_id\": content_id}\n","        ))\n","    \n","    if points:\n","        print(f\"Upserting {len(points)} points to Qdrant.\")\n","        # Upsert the points into the Qdrant collection named \"image_search\"\n","        client.upsert(\n","            collection_name=\"image_search\",\n","            points=points\n","        )\n","    else:\n","        print(\"No points to upsert, skipping request.\")"]},{"cell_type":"markdown","metadata":{},"source":["4. **Process a dataset of images and store their features in the Qdrant vector store.**\n","- This task involves iterating through a dataset of images, extracting features from each image using tools like Indexify, and then storing these extracted features in the Qdrant vector store. This process enables efficient management and retrieval of image data for tasks such as content-based image retrieval and similarity searches."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def process_dataset(dataset_folder):\n","    \"\"\"\n","    Process a dataset of images and store their features in the Qdrant vector store.\n","\n","    Parameters:\n","    dataset_folder (str): The path to the dataset folder containing image files.\n","    \"\"\"\n","    # Walk through the dataset folder and its subdirectories\n","    for root, subdirs, files in os.walk(dataset_folder):\n","        for subdir in subdirs:\n","            subdir_path = os.path.join(root, subdir)\n","            print(f\"Processing folder: {subdir_path}\")\n","\n","            # Iterate through each file in the subdirectory\n","            for filename in os.listdir(subdir_path):\n","                file_path = os.path.join(subdir_path, filename)\n","                try:\n","                    # Extract features from the image file\n","                    features, content_id = extract_image_features(file_path)\n","                    if not features:\n","                        print(f\"No features extracted for {file_path}\")\n","\n","                    # Store the extracted features in the Qdrant vector store\n","                    store_features_in_vector_store(features, content_id)\n","                except Exception as e:\n","                    print(f\"Failed to process {file_path}: {e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["# Example usage\n","dataset_folder = \"/workspace/ml-projects/object-detection-reverse-image-search-indexify/data/natural_images\"\n","process_dataset(dataset_folder)"]},{"cell_type":"markdown","metadata":{},"source":["### **Reverse Image Search Function**\n","\n","#### **Perform reverse image search using Qdrant and retrieve original contents from Indexify.**\n","\n","- Perform reverse image search using Qdrant for vector similarity search and retrieve detailed content information from Indexify based on matched images."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def reverse_image_search(query_image_path):\n","    \"\"\"\n","    Perform reverse image search using Qdrant and retrieve original contents from Indexify.\n","\n","    Parameters:\n","    query_image_path (str): Path to the query image.\n","\n","    Returns:\n","    list: List of original contents corresponding to similar images found.\n","    \"\"\"\n","    features, query_content_id = extract_image_features(query_image_path)\n","\n","    # Extract the vector representation from the features\n","    vectors = []\n","    for feature in features:\n","        feature_content = feature['content']\n","        feature_data = json.loads(feature_content.decode('utf-8'))\n","        bbox = feature_data['bbox']\n","        score = feature_data['score']\n","\n","        # Create a vector representation combining bbox and score\n","        vector = bbox + [score]  # Assuming bbox is a list of numbers\n","        vectors.append(vector)\n","\n","    # Assuming we take the first vector for the query\n","    query_vector = vectors[0]\n","\n","    # Query the vector store\n","    qdrant_client = QdrantClient(\"http://localhost:6333\")\n","    search_result = qdrant_client.search(\n","        collection_name=\"image_search\",\n","        query_vector=query_vector,\n","        limit=5  # Number of similar images to retrieve\n","    )\n","\n","    # Extract content_id from the search results\n","    content_ids = [point.payload['content_id'] for point in search_result]\n","\n","    # Retrieve the original content from Indexify using content_id\n","    indexify_client = IndexifyClient()\n","    original_contents = []\n","    for content_id in content_ids:\n","        content = indexify_client.get_extracted_content(\n","            content_id=content_id,\n","            graph_name=\"yolo_image_detector\",\n","            policy_name=\"image_object_detection\"\n","        )\n","        original_contents.append(content)\n","\n","    return original_contents"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["if __name__ == \"__main__\":\n","    query_image_path = 'query_image.jpg'\n","    results = reverse_image_search(query_image_path)\n","    # Print the results in a neat format\n","    for result_set in results:\n","        for result in result_set:\n","            print(f\"ID: {result['id']}\")\n","            content_data = json.loads(result['content'].decode('utf-8'))\n","            bbox = content_data['bbox']\n","            class_name = content_data['class']\n","            score = content_data['score']\n","            print(f\"Class: {class_name}, BBox: {bbox}, Score: {score}\")\n","        print()  # Empty line between different result"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":42780,"sourceId":75676,"sourceType":"datasetVersion"},{"datasetId":5398399,"sourceId":8967658,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelInstanceId":65500,"sourceId":77915,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
